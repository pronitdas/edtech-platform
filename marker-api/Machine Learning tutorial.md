"picture-1.png\n\n## MLU-EXPLAIN\n\nVisual explanations of core machine learning concepts\n\n## Machine Learning\\_University \\_(MLU)\n\nis an education initiative from Amazon designed to teach machine learning theory and practical application.\n\nAs of that goal, MLU-Explain exists to teach important machine learning concepts through visual essays in a fun, informative; and accessible manner. part\n\n## Explore Published Articles\n\npicture-2.png\n\nNEURAL\n\nNETWORKS\n\npicture-3.png\n\n## EQUALITYOF ODDS\n\nExplore equality of odds, a metric used to quantify unfairness and remove bias\n\npicture-4.png\n\nLearn about neural networks, the backbone of many popular algorithms such as ChatGPT, Stable Diffusion; and many others today\n\npicture-5.png\n\npicture-6.png\n\npicture-7.png\n\n## LOGISTIC\n\n## LINEAR\n\n## REGRESSION\n\n## REGRESSION\n\nLearn how logistic regression can be used for binary classification in machine learning through an interactive example.\n\nInteractively learn about linear regression models as they're commonly used in the context of machine learning.\n\npicture-8.png\n\npicture-9.png\n\npicture-10.png\n\npicture-11.png\n\nfrom machine learning models.\n\npicture-12.png\n\n## REINFORCEMENT\n\n## ROC & AUC\n\n## LEARNING\n\nLearn about Reinforcement Learning (RL) and the exploration-exploitation dilemma with this interactive article.\n\npicture-13.png\n\npicture-14.png\n\npicture-15.png\n\n## CROSS-VALIDATION\n\nK-Fold Cross-Validation: a resampling technique to improve estimates of test error rates compared to a simple validation set. help\n\npicture-16.png\n\n## IRAIN,\\_TESI,\\_AND VALIDATION SETS\n\nLearn why it is best practice to split your data into training; testing; and validation sets, and explore the utility of each with a live machine learning model.\n\npicture-17.png\n\nA visual explanation of the Receiver Operating Characteristic Curve (ROC) curve; how it works with a live interactive example; and how it relates to Area Under The Curve (AUC)\n\npicture-18.png\n\npicture-19.png\n\npicture-20.png\n\n## PRECISION &\n\n## RANDOM FOREST\n\n## RECALL\n\nWhen it comes to evaluating classification models, accuracy is often a poor metric. This article covers two common alternatives, Precision and Recall, as well as the FI-score and Confusion Matrices.\n\npicture-21.png\n\npicture-22.png\n\n## DECISION TREES\n\nTHE BIAS VARIANCE\n\nExplore one of machine learning's most popular supervised algorithms: the Decision Tree. Learn how the tree makes its splits, the\n\n## TRADEOFF\n\nUnderstand the tradeoff between under- and over fitting models, how it relates to bias and variance; and\n\nLearn how the majority vote and well-placed randomness can extend the decision tree model to one of machine learning's most widely-used algorithms  the Random Forest.\n\npicture-23.png\n\npicture-24.png\n\nconcepts of Entropy and Information Gain; and why going too deep is problematic.\n\npicture-25.png\n\npicture-26.png\n\n## DOUBLE DESCENT: VISUAL INTRODUCTION\n\n## DOUBLE DESCENT: MATHEMATICAL EXPLANATION\n\nMeet the double descent phenomenon in modern machine learning: what it is, how it relates to the bias variance tradeoff, the importance of the interpolation regime; and a theory of what lies behind.\n\nDeepen your understanding of the double descent phenomenon:. The article builds on the cubic spline example introduced in Double Descent 1, describing in mathematical detail what is happening.\n\nDive In\n\nDive In\n\nexplore interactive examples related to LOESS and KNN.\n\npicture-27.png\n\npicture-28.png"