from fastapi import FastAPI, Request, Depends, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
import httpx
import os
import json
import time
import logging
from typing import Dict, List, Optional, Any, Union
import asyncio
from pydantic import BaseModel, Field
import prometheus_client
from prometheus_client import Counter, Histogram, Gauge
import structlog

# Initialize logging
logger = structlog.get_logger()

# Setup metrics
REQUEST_COUNT = Counter('llm_request_count', 'Count of requests by model and endpoint', ['model', 'endpoint'])
LATENCY = Histogram('llm_request_latency_seconds', 'Latency of requests by model', ['model'])
MODEL_TOKENS = Counter('llm_tokens_generated', 'Tokens generated by model', ['model'])
MODEL_LOAD = Gauge('llm_model_loaded', 'Whether a model is currently loaded', ['model'])

# Initialize the FastAPI app
app = FastAPI(
    title="LLM Gateway API",
    description="API Gateway for routing requests to multiple LLM models",
    version="1.0.0"
)

# Enable CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, replace with specific origins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Environment variables
MODEL_SERVERS = {
    server.split(':')[0]: server.split(':')[1] 
    for server in os.getenv('MODEL_SERVERS', 'llama-7b:http://llama-7b:8000').split(',')
}
DEFAULT_MODEL = os.getenv('DEFAULT_MODEL', 'llama-7b')
API_KEY = os.getenv('API_KEY', 'default_dev_key')
MAX_CONCURRENT = int(os.getenv('MAX_CONCURRENT_REQUESTS', '20'))
REQUEST_TIMEOUT = int(os.getenv('REQUEST_TIMEOUT', '120'))

# Semaphore to limit concurrent requests
REQUEST_SEMAPHORE = asyncio.Semaphore(MAX_CONCURRENT)

# Models
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str = Field(default=DEFAULT_MODEL)
    messages: List[ChatMessage]
    temperature: Optional[float] = 0.7
    top_p: Optional[float] = 1.0
    max_tokens: Optional[int] = 1024
    stream: Optional[bool] = False
    stop: Optional[Union[str, List[str]]] = None

class GenerationRequest(BaseModel):
    model: str = Field(default=DEFAULT_MODEL)
    prompt: str
    temperature: Optional[float] = 0.7
    top_p: Optional[float] = 1.0
    max_tokens: Optional[int] = 1024
    stream: Optional[bool] = False
    stop: Optional[Union[str, List[str]]] = None

class ModelList(BaseModel):
    models: List[str]

# Authentication dependency
async def verify_api_key(request: Request):
    api_key = request.headers.get("Authorization")
    if not api_key:
        api_key = request.query_params.get("api_key")
    
    if not api_key:
        raise HTTPException(status_code=401, detail="API key is required")
    
    # Remove Bearer prefix if present
    api_key = api_key.replace("Bearer ", "")
    
    if api_key != API_KEY:
        raise HTTPException(status_code=403, detail="Invalid API key")
    
    return api_key

# Utility functions
async def forward_request(model: str, endpoint: str, json_data: Dict[str, Any], stream: bool = False):
    """Forward a request to the appropriate model server."""
    if model not in MODEL_SERVERS:
        raise HTTPException(status_code=404, detail=f"Model {model} not found")
    
    server_url = MODEL_SERVERS[model]
    url = f"{server_url}/{endpoint}"
    
    try:
        async with REQUEST_SEMAPHORE:
            async with httpx.AsyncClient(timeout=REQUEST_TIMEOUT) as client:
                if stream:
                    response = await client.post(url, json=json_data, headers={"Accept": "text/event-stream"})
                    return response
                else:
                    start_time = time.time()
                    response = await client.post(url, json=json_data)
                    end_time = time.time()
                    LATENCY.labels(model=model).observe(end_time - start_time)
                    
                    return response.json()
    except httpx.RequestError as e:
        logger.error(f"Error forwarding request to {url}: {e}")
        raise HTTPException(status_code=503, detail=f"Model server {model} is unavailable: {str(e)}")
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        raise HTTPException(status_code=500, detail=f"Unexpected error: {str(e)}")

# Routes
@app.get("/health")
async def health_check():
    """Health check endpoint."""
    return {"status": "healthy"}

@app.get("/metrics")
async def metrics():
    """Endpoint for Prometheus metrics."""
    return StreamingResponse(
        prometheus_client.generate_latest(),
        media_type="text/plain"
    )

@app.get("/v1/models", response_model=ModelList)
async def list_models(_: str = Depends(verify_api_key)):
    """List available models."""
    return ModelList(models=list(MODEL_SERVERS.keys()))

@app.post("/v1/chat/completions")
async def chat_completions(
    request: ChatCompletionRequest,
    _: str = Depends(verify_api_key)
):
    """Route for chat completions API."""
    REQUEST_COUNT.labels(model=request.model, endpoint="chat/completions").inc()
    
    # Format request for llama.cpp server
    formatted_request = {
        "prompt": "",  # We'll construct this from messages
        "temperature": request.temperature,
        "top_p": request.top_p,
        "max_tokens": request.max_tokens,
        "stream": request.stream,
    }
    
    # Construct prompt from messages
    prompt = ""
    for msg in request.messages:
        if msg.role == "system":
            prompt += f"<s>[INST] <<SYS>>\n{msg.content}\n<</SYS>>\n\n"
        elif msg.role == "user":
            # If there was a system message, continue that instruction, otherwise start a new one
            if not prompt:
                prompt += f"<s>[INST] {msg.content} [/INST]"
            else:
                prompt += f"{msg.content} [/INST]"
        elif msg.role == "assistant":
            prompt += f" {msg.content} </s>"
            # If another user message follows, we need to start a new instruction
            if request.messages.index(msg) < len(request.messages) - 1 and request.messages[request.messages.index(msg) + 1].role == "user":
                prompt += f"<s>[INST] "
    
    # Handle stop sequences
    if request.stop:
        if isinstance(request.stop, list):
            formatted_request["stop"] = request.stop
        else:
            formatted_request["stop"] = [request.stop]
    
    formatted_request["prompt"] = prompt
    
    # Handle streaming
    if request.stream:
        response = await forward_request(request.model, "completion", formatted_request, stream=True)
        return StreamingResponse(
            content=response.aiter_text(),
            media_type="text/event-stream"
        )
    
    # Non-streaming request
    response = await forward_request(request.model, "completion", formatted_request)
    return response

@app.post("/v1/completions")
async def completions(
    request: GenerationRequest,
    _: str = Depends(verify_api_key)
):
    """Route for text completions API."""
    REQUEST_COUNT.labels(model=request.model, endpoint="completions").inc()
    
    # Format request for llama.cpp server
    formatted_request = {
        "prompt": request.prompt,
        "temperature": request.temperature,
        "top_p": request.top_p,
        "max_tokens": request.max_tokens,
        "stream": request.stream,
    }
    
    # Handle stop sequences
    if request.stop:
        if isinstance(request.stop, list):
            formatted_request["stop"] = request.stop
        else:
            formatted_request["stop"] = [request.stop]
    
    # Handle streaming
    if request.stream:
        response = await forward_request(request.model, "completion", formatted_request, stream=True)
        return StreamingResponse(
            content=response.aiter_text(),
            media_type="text/event-stream"
        )
    
    # Non-streaming request
    response = await forward_request(request.model, "completion", formatted_request)
    return response

# Startup and shutdown events
@app.on_event("startup")
async def startup_event():
    """Run when the server starts."""
    logger.info("Starting up the LLM Gateway...")
    for model in MODEL_SERVERS:
        MODEL_LOAD.labels(model=model).set(1)
    logger.info(f"Loaded models: {list(MODEL_SERVERS.keys())}")

@app.on_event("shutdown")
async def shutdown_event():
    """Run when the server shuts down."""
    logger.info("Shutting down the LLM Gateway...")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080) 